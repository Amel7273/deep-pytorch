{"cells":[{"cell_type":"code","execution_count":1,"id":"958ae5c6","metadata":{"id":"958ae5c6","executionInfo":{"status":"ok","timestamp":1642478936375,"user_tz":-540,"elapsed":5,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[],"source":["#10.1.1 희소표현(Sparse Representation)"]},{"cell_type":"code","source":["from google.colab import files # 데이터 불러오기\n","file_uploaded=files.upload()   # chap10/data/class2.csv 데이터 불러오기"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":77},"id":"8p2aHzC0-6wu","outputId":"13fb7b2f-4e02-4fb7-9cd1-4b076a77cdf2","executionInfo":{"status":"ok","timestamp":1642478960680,"user_tz":-540,"elapsed":15713,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"id":"8p2aHzC0-6wu","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-d533641f-b853-4b5a-8680-b10d8f6fb109\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d533641f-b853-4b5a-8680-b10d8f6fb109\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving class2.csv to class2.csv\n"]}]},{"cell_type":"code","execution_count":3,"id":"d93e9e0d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d93e9e0d","outputId":"7138eb49-5528-4db9-e7b5-84b615b1005e","executionInfo":{"status":"ok","timestamp":1642478972336,"user_tz":-540,"elapsed":5697,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 1, 0, 1, 0])"]},"metadata":{},"execution_count":3}],"source":["import pandas as pd\n","import torch\n","\n","class2=pd.read_csv(\"class2.csv\")\n","\n","from sklearn import preprocessing \n","label_encoder = preprocessing.LabelEncoder()\n","onehot_encoder = preprocessing.OneHotEncoder()\n","\n","train_x = label_encoder.fit_transform(class2['class2'])\n","train_x"]},{"cell_type":"code","execution_count":null,"id":"6af2eeeb","metadata":{"id":"6af2eeeb"},"outputs":[],"source":["#10.1.2 횟수기반 임베딩\n","#Counter Vector"]},{"cell_type":"code","execution_count":4,"id":"d5298928","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5298928","outputId":"d03937a8-6f1a-4f85-b575-8709812a0d35","executionInfo":{"status":"ok","timestamp":1642478974650,"user_tz":-540,"elapsed":296,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'and': 0,\n"," 'any': 1,\n"," 'chance': 2,\n"," 'do': 3,\n"," 'get': 4,\n"," 'have': 5,\n"," 'if': 6,\n"," 'is': 7,\n"," 'last': 8,\n"," 'never': 9,\n"," 'not': 10,\n"," 'one': 11,\n"," 'please': 12,\n"," 'this': 13,\n"," 'will': 14,\n"," 'you': 15}"]},"metadata":{},"execution_count":4}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","corpus = [\n","    'This is last chance.',\n","    'and if you do not have this chance.',\n","    'you will never get any chance.',\n","    'will you do get this one?',\n","    'please, get this chance',\n","]\n","vect = CountVectorizer()\n","vect.fit(corpus)\n","vect.vocabulary_"]},{"cell_type":"code","execution_count":5,"id":"cda3fe3e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cda3fe3e","outputId":"3238f28a-b732-4fd8-f1d3-820a6956e79e","executionInfo":{"status":"ok","timestamp":1642478977008,"user_tz":-540,"elapsed":3,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"]},"metadata":{},"execution_count":5}],"source":["vect.transform(['you will never get any chance.']).toarray()"]},{"cell_type":"code","execution_count":6,"id":"6b305aef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b305aef","outputId":"58f6bca6-b280-41ab-bdf0-feb18423b949","executionInfo":{"status":"ok","timestamp":1642478978460,"user_tz":-540,"elapsed":266,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'any': 0,\n"," 'chance': 1,\n"," 'do': 2,\n"," 'get': 3,\n"," 'have': 4,\n"," 'if': 5,\n"," 'last': 6,\n"," 'never': 7,\n"," 'not': 8,\n"," 'one': 9,\n"," 'will': 10,\n"," 'you': 11}"]},"metadata":{},"execution_count":6}],"source":["vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus)\n","vect.vocabulary_"]},{"cell_type":"code","execution_count":null,"id":"d212c4d7","metadata":{"id":"d212c4d7"},"outputs":[],"source":["#TF-IDF"]},{"cell_type":"code","execution_count":7,"id":"8fea52e5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fea52e5","outputId":"96533f61-1d4e-4340-e0c7-c0855703018a","executionInfo":{"status":"ok","timestamp":1642478980306,"user_tz":-540,"elapsed":270,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["유사도를 위한 3 x 3 matrix를 만들었습니다.\n","[[1.       0.224325 0.      ]\n"," [0.224325 1.       0.      ]\n"," [0.       0.       1.      ]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n","tfidf_vectorizer = TfidfVectorizer(min_df=1)\n","tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n","doc_distance = (tfidf_matrix * tfidf_matrix.T)\n","print ('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), 'matrix를 만들었습니다.')\n","print(doc_distance.toarray())"]},{"cell_type":"code","execution_count":null,"id":"52460393","metadata":{"id":"52460393"},"outputs":[],"source":["#10.1.3 예측기반 임베딩\n","# Word2Vec"]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmcOz0VU_ROY","outputId":"d36f29af-e697-441b-a090-c7fa338a0d48","executionInfo":{"status":"ok","timestamp":1642478986313,"user_tz":-540,"elapsed":3183,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"id":"wmcOz0VU_ROY","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"popular\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RL2E_RP3_SB0","outputId":"e84e207f-679f-472a-e965-4c26a570f736","executionInfo":{"status":"ok","timestamp":1642478994406,"user_tz":-540,"elapsed":6213,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"id":"RL2E_RP3_SB0","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from google.colab import files # 데이터 불러오기\n","file_uploaded=files.upload()   # chap10/data/peter.txt 데이터 불러오기"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":77},"id":"DlANtiez_b0r","outputId":"f6505f47-35de-4f13-ff9d-11b401525bd2","executionInfo":{"status":"ok","timestamp":1642479003510,"user_tz":-540,"elapsed":6628,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"id":"DlANtiez_b0r","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-f58fa4ca-7488-4676-b953-fa5b82cc6ac2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-f58fa4ca-7488-4676-b953-fa5b82cc6ac2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving peter.txt to peter.txt\n"]}]},{"cell_type":"code","execution_count":11,"id":"d68eed27","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d68eed27","outputId":"083f5012-f7d1-4a1f-c55f-b959350de0ca","executionInfo":{"status":"ok","timestamp":1642479005792,"user_tz":-540,"elapsed":781,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['once',\n","  'upon',\n","  'a',\n","  'time',\n","  'in',\n","  'london',\n","  ',',\n","  'the',\n","  'darlings',\n","  'went',\n","  'out',\n","  'to',\n","  'a',\n","  'dinner',\n","  'party',\n","  'leaving',\n","  'their',\n","  'three',\n","  'children',\n","  'wendy',\n","  ',',\n","  'jhon',\n","  ',',\n","  'and',\n","  'michael',\n","  'at',\n","  'home',\n","  '.'],\n"," ['after',\n","  'wendy',\n","  'had',\n","  'tucked',\n","  'her',\n","  'younger',\n","  'brothers',\n","  'jhon',\n","  'and',\n","  'michael',\n","  'to',\n","  'bed',\n","  ',',\n","  'she',\n","  'went',\n","  'to',\n","  'read',\n","  'a',\n","  'book',\n","  '.'],\n"," ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'],\n"," ['he', 'was', 'flying', '.'],\n"," ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'],\n"," ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'],\n"," ['“', 'hello', '!'],\n"," ['who', 'are', 'you', '?'],\n"," ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'],\n"," ['“', 'my', 'name', 'is', 'peter', 'pan', '.'],\n"," ['my',\n","  'shadow',\n","  'wouldn',\n","  '’',\n","  't',\n","  'stock',\n","  'to',\n","  'me.',\n","  '”',\n","  ',',\n","  'he',\n","  'replied',\n","  '.'],\n"," ['she', 'asked', 'him', 'to', 'come', 'in', '.'],\n"," ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'],\n"," ['wendy',\n","  'took',\n","  'his',\n","  'shadow',\n","  'and',\n","  'sewed',\n","  'it',\n","  'to',\n","  'his',\n","  'shoe',\n","  'tips',\n","  '.'],\n"," ['now',\n","  'his',\n","  'shadow',\n","  'followed',\n","  'him',\n","  'wherever',\n","  'peter',\n","  'pan',\n","  'went',\n","  '!'],\n"," ['he',\n","  'was',\n","  'delighted',\n","  'and',\n","  'asked',\n","  'wendy',\n","  '“',\n","  'why',\n","  'don',\n","  '’',\n","  't',\n","  'you',\n","  'come',\n","  'with',\n","  'me',\n","  'to',\n","  'my',\n","  'home',\n","  '.'],\n"," ['the', 'neverland', '.'],\n"," ['i',\n","  'lived',\n","  'there',\n","  'with',\n","  'my',\n","  'fairy',\n","  'tinker',\n","  'bell.',\n","  '”',\n","  'wendy',\n","  '?'],\n"," ['“', 'oh', '!'],\n"," ['what', 'a', 'wonderful', 'idea', '!'],\n"," ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'],\n"," ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'],\n"," ['“', 'yes', '!'],\n"," ['of', 'course', '!'],\n"," ['get',\n","  'them',\n","  'we',\n","  'will',\n","  'all',\n","  'fly',\n","  'together.',\n","  '”',\n","  'peter',\n","  'pan',\n","  'replied',\n","  'and',\n","  'so',\n","  'it',\n","  'was',\n","  '.'],\n"," ['five',\n","  'little',\n","  'figures',\n","  'flew',\n","  'out',\n","  'of',\n","  'the',\n","  'window',\n","  'of',\n","  'the',\n","  'darlings',\n","  'and',\n","  'headed',\n","  'towards',\n","  'neverland',\n","  '.'],\n"," ['as',\n","  'they',\n","  'flew',\n","  'over',\n","  'the',\n","  'island',\n","  ',',\n","  'peter',\n","  'pan',\n","  'told',\n","  'the',\n","  'children',\n","  'more',\n","  'about',\n","  'his',\n","  'homeland',\n","  '.'],\n"," ['“',\n","  'all',\n","  'the',\n","  'children',\n","  'who',\n","  'get',\n","  'lost',\n","  'come',\n","  'and',\n","  'stay',\n","  'with',\n","  'tinker',\n","  'bell',\n","  'and',\n","  'me',\n","  ',',\n","  '”',\n","  'peter',\n","  'told',\n","  'them',\n","  '.'],\n"," ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'],\n"," ['the',\n","  'mermaids',\n","  'live',\n","  'in',\n","  'the',\n","  'lagoon',\n","  'around',\n","  'the',\n","  'island',\n","  '.'],\n"," ['and',\n","  'a',\n","  'very',\n","  'mean',\n","  'pirate',\n","  'called',\n","  'captain',\n","  'hook',\n","  'keeps',\n","  'troubling',\n","  'everyone',\n","  '.'],\n"," ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'],\n"," ['so',\n","  'the',\n","  'captain',\n","  'had',\n","  'to',\n","  'put',\n","  'a',\n","  'hook',\n","  'in',\n","  'its',\n","  'place',\n","  '.'],\n"," ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'],\n"," ['and', 'rightly', 'so', '!'],\n"," ['if',\n","  'the',\n","  'crocodile',\n","  'ever',\n","  'found',\n","  'captain',\n","  'hook',\n","  'it',\n","  'will',\n","  'eat',\n","  'up',\n","  'the',\n","  'rest',\n","  'of',\n","  'it',\n","  'couldn',\n","  '’',\n","  't',\n","  'eat',\n","  'last',\n","  'time.',\n","  '”',\n","  'peter',\n","  'told',\n","  'them',\n","  '.'],\n"," ['soon', 'they', 'landed', 'on', 'the', 'island', '.'],\n"," ['and',\n","  'to',\n","  'the',\n","  'surprise',\n","  'of',\n","  'wendy',\n","  ',',\n","  'jhon',\n","  'and',\n","  'michael',\n","  ',',\n","  'peter',\n","  'pan',\n","  'let',\n","  'them',\n","  'in',\n","  'through',\n","  'a',\n","  'small',\n","  'opening',\n","  'in',\n","  'a',\n","  'tree',\n","  '.'],\n"," ['inside',\n","  'the',\n","  'tree',\n","  'was',\n","  'a',\n","  'large',\n","  'room',\n","  'with',\n","  'children',\n","  'inside',\n","  'it',\n","  '.'],\n"," ['somewhere',\n","  'huddled',\n","  'by',\n","  'the',\n","  'fire',\n","  'in',\n","  'the',\n","  'corner',\n","  'and',\n","  'somewhere',\n","  'playing',\n","  'amongst',\n","  'themselves',\n","  '.'],\n"," ['their',\n","  'faces',\n","  'lit',\n","  'up',\n","  'when',\n","  'they',\n","  'saw',\n","  'peter',\n","  'pan',\n","  ',',\n","  'tinker',\n","  'bell',\n","  ',',\n","  'and',\n","  'their',\n","  'guests',\n","  '.'],\n"," ['“', 'hello', 'everyone', '.'],\n"," ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n"," ['they',\n","  'will',\n","  'be',\n","  'staying',\n","  'with',\n","  'us',\n","  'from',\n","  'now',\n","  'on.',\n","  '”',\n","  'peter',\n","  'pan',\n","  'introduced',\n","  'them',\n","  'to',\n","  'all',\n","  'children',\n","  '.'],\n"," ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n"," ['a', 'few', 'days', 'passed', '.'],\n"," ['and', 'they', 'settled', 'into', 'a', 'routine', '.'],\n"," ['wendy',\n","  'would',\n","  'take',\n","  'care',\n","  'of',\n","  'all',\n","  'the',\n","  'children',\n","  'in',\n","  'the',\n","  'day',\n","  'and',\n","  'would',\n","  'go',\n","  'out',\n","  'with',\n","  'peter',\n","  'pan',\n","  'and',\n","  'her',\n","  'brothers',\n","  'in',\n","  'the',\n","  'evening',\n","  'to',\n","  'learn',\n","  'about',\n","  'the',\n","  'island',\n","  '.'],\n"," ['she',\n","  'would',\n","  'cook',\n","  'for',\n","  'them',\n","  'and',\n","  'stitch',\n","  'new',\n","  'clothes',\n","  'for',\n","  'them',\n","  '.'],\n"," ['he',\n","  'even',\n","  'made',\n","  'a',\n","  'lovely',\n","  'new',\n","  'dress',\n","  'for',\n","  'tinker',\n","  'bell',\n","  '.'],\n"," ['one',\n","  'evening',\n","  ',',\n","  'as',\n","  'they',\n","  'were',\n","  'out',\n","  'exploring',\n","  'the',\n","  'island',\n","  'peter',\n","  'pan',\n","  'warned',\n","  'everyone',\n","  'and',\n","  'said',\n","  ',',\n","  '“',\n","  'hide',\n","  '!'],\n"," ['hide', '!'],\n"," ['pirates', '!'],\n"," ['and',\n","  'they',\n","  'have',\n","  'kidnapped',\n","  'the',\n","  'indian',\n","  'princess',\n","  'tiger',\n","  'lily',\n","  '.'],\n"," ['they',\n","  'have',\n","  'kept',\n","  'her',\n","  'there',\n","  ',',\n","  'tied',\n","  'up',\n","  'by',\n","  'the',\n","  'rocks',\n","  ',',\n","  'near',\n","  'the',\n","  'water.',\n","  '”',\n","  'peter',\n","  'was',\n","  'afraid',\n","  'and',\n","  'the',\n","  'princess',\n","  'would',\n","  'drown',\n","  ',',\n","  'is',\n","  'she',\n","  'fell',\n","  'into',\n","  'the',\n","  'water',\n","  '.'],\n"," ['so',\n","  ',',\n","  'in',\n","  'a',\n","  'voice',\n","  'that',\n","  'sounded',\n","  'like',\n","  'captain',\n","  'hook',\n","  ',',\n","  'he',\n","  'shouted',\n","  'instructions',\n","  'to',\n","  'the',\n","  'pirates',\n","  'who',\n","  'guarded',\n","  'her',\n","  ',',\n","  '“',\n","  'you',\n","  'fools',\n","  '!'],\n"," ['let', 'her', 'go', 'at', 'once', '!'],\n"," ['do',\n","  'it',\n","  'before',\n","  'i',\n","  'come',\n","  'there',\n","  'or',\n","  'else',\n","  'i',\n","  'will',\n","  'throw',\n","  'each',\n","  'one',\n","  'of',\n","  'you',\n","  'into',\n","  'the',\n","  'water.',\n","  '”',\n","  'the',\n","  'pirates',\n","  'got',\n","  'scared',\n","  'and',\n","  'immediately',\n","  'released',\n","  'the',\n","  'princes',\n","  '.'],\n"," ['she',\n","  'quickly',\n","  'dived',\n","  'into',\n","  'the',\n","  'water',\n","  'and',\n","  'swam',\n","  'to',\n","  'the',\n","  'safety',\n","  'of',\n","  'her',\n","  'home',\n","  '.'],\n"," ['soon',\n","  'everyone',\n","  'found',\n","  'out',\n","  'how',\n","  'peter',\n","  'pan',\n","  'had',\n","  'rescued',\n","  'the',\n","  'princess',\n","  '.'],\n"," ['when',\n","  'captain',\n","  'hook',\n","  'found',\n","  'out',\n","  'how',\n","  'peter',\n","  'had',\n","  'tricked',\n","  'his',\n","  'men',\n","  'he',\n","  'was',\n","  'furious',\n","  '.'],\n"," ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'],\n"," ['that',\n","  'night',\n","  'wendy',\n","  'told',\n","  'peter',\n","  'pan',\n","  ',',\n","  'that',\n","  'she',\n","  'and',\n","  'her',\n","  'brother',\n","  'wanted',\n","  'to',\n","  'go',\n","  'back',\n","  'home',\n","  'since',\n","  'they',\n","  'missed',\n","  'their',\n","  'parents',\n","  '.'],\n"," ['she',\n","  'said',\n","  'if',\n","  'the',\n","  'lost',\n","  'children',\n","  'could',\n","  'also',\n","  'return',\n","  'to',\n","  'her',\n","  'world',\n","  'they',\n","  'could',\n","  'find',\n","  'a',\n","  'nice',\n","  'home',\n","  'for',\n","  'them',\n","  '.'],\n"," ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'],\n"," ['but',\n","  'the',\n","  'sake',\n","  'of',\n","  'the',\n","  'lost',\n","  'children',\n","  'he',\n","  'agreed',\n","  ',',\n","  'although',\n","  'a',\n","  'bit',\n","  'sadly',\n","  '.'],\n"," ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'],\n"," ['the',\n","  'next',\n","  'morning',\n","  'all',\n","  'the',\n","  'lost',\n","  'children',\n","  'left',\n","  'with',\n","  'wendy',\n","  ',',\n","  'jhon',\n","  ',',\n","  'and',\n","  'michael',\n","  '.'],\n"," ['but',\n","  'on',\n","  'the',\n","  'way',\n","  ',',\n","  'captain',\n","  'hook',\n","  'and',\n","  'his',\n","  'men',\n","  'kidnapped',\n","  'all',\n","  'of',\n","  'them',\n","  '.'],\n"," ['he',\n","  'tied',\n","  'them',\n","  'and',\n","  'kept',\n","  'them',\n","  'on',\n","  'once',\n","  'of',\n","  'his',\n","  'ships',\n","  '.'],\n"," ['as',\n","  'soon',\n","  'as',\n","  'peter',\n","  'found',\n","  'out',\n","  'about',\n","  'it',\n","  'he',\n","  'rushed',\n","  'to',\n","  'the',\n","  'ship',\n","  '.'],\n"," ['he',\n","  'swung',\n","  'himself',\n","  'from',\n","  'a',\n","  'tress',\n","  'branch',\n","  'and',\n","  'on',\n","  'to',\n","  'the',\n","  'deck',\n","  'of',\n","  'the',\n","  'ship',\n","  'where',\n","  'all',\n","  'the',\n","  'children',\n","  'were',\n","  'tied',\n","  'up',\n","  '.'],\n"," ['he',\n","  'swung',\n","  'his',\n","  'sword',\n","  'bravely',\n","  'and',\n","  'threw',\n","  'over',\n","  'the',\n","  'pirates',\n","  'who',\n","  'tried',\n","  'to',\n","  'stop',\n","  'him',\n","  '.'],\n"," ['quickly',\n","  'he',\n","  'released',\n","  'everyone',\n","  'from',\n","  'their',\n","  'captor',\n","  '’',\n","  's',\n","  'ties',\n","  '.'],\n"," ['wendy',\n","  ',',\n","  'jhon',\n","  ',',\n","  'michael',\n","  'and',\n","  'tinker',\n","  'bell',\n","  'helped',\n","  'all',\n","  'the',\n","  'children',\n","  'into',\n","  'the',\n","  'water',\n","  ',',\n","  'where',\n","  'their',\n","  'friends',\n","  'from',\n","  'the',\n","  'indian',\n","  'camp',\n","  'were',\n","  'ready',\n","  'with',\n","  'smaller',\n","  'boats',\n","  'to',\n","  'take',\n","  'them',\n","  'to',\n","  'safety',\n","  'peter',\n","  'pan',\n","  'now',\n","  'went',\n","  'looking',\n","  'for',\n","  'captain',\n","  'hook',\n","  '.'],\n"," ['“',\n","  'let',\n","  'us',\n","  'finished',\n","  'this',\n","  'forever',\n","  'mr.',\n","  'hook',\n","  '”',\n","  ',',\n","  'peter',\n","  'challenged',\n","  'captain',\n","  'hook',\n","  '.'],\n"," ['“', 'yes', '!'],\n"," ['peter',\n","  'pan',\n","  ',',\n","  'you',\n","  'have',\n","  'caused',\n","  'me',\n","  'enough',\n","  'trouble',\n","  '.'],\n"," ['it',\n","  'is',\n","  'time',\n","  'that',\n","  'we',\n","  'finished',\n","  'this.',\n","  '”',\n","  'hook',\n","  'replied',\n","  '.'],\n"," ['with',\n","  'his',\n","  'sword',\n","  'drawn',\n","  ',',\n","  'he',\n","  'raced',\n","  'towards',\n","  'peter',\n","  'pan',\n","  '.'],\n"," ['quick',\n","  'on',\n","  'his',\n","  'feet',\n","  ',',\n","  'peter',\n","  'pan',\n","  'stepped',\n","  'aside',\n","  'and',\n","  'pushed',\n","  'hook',\n","  'inside',\n","  'the',\n","  'sea',\n","  'where',\n","  'the',\n","  'crocodile',\n","  'was',\n","  'waiting',\n","  'to',\n","  'eat',\n","  'the',\n","  'rest',\n","  'of',\n","  'hook',\n","  '.'],\n"," ['everyone',\n","  'rejoiced',\n","  'as',\n","  'captain',\n","  'hook',\n","  'was',\n","  'out',\n","  'of',\n","  'their',\n","  'lives',\n","  'forever',\n","  '.'],\n"," ['everybody', 'headed', 'back', 'to', 'london', '.'],\n"," ['mr.', 'and', 'mrs', '.'],\n"," ['darling',\n","  'was',\n","  'so',\n","  'happy',\n","  'to',\n","  'see',\n","  'their',\n","  'children',\n","  'and',\n","  'they',\n","  'agreed',\n","  'to',\n","  'adopt',\n","  'the',\n","  'lost',\n","  'children',\n","  '.'],\n"," ['they',\n","  'even',\n","  'asked',\n","  'peter',\n","  'pan',\n","  'to',\n","  'come',\n","  'and',\n","  'live',\n","  'with',\n","  'them',\n","  '.'],\n"," ['but',\n","  'peter',\n","  'pan',\n","  'said',\n","  ',',\n","  'he',\n","  'never',\n","  'wanted',\n","  'to',\n","  'grow',\n","  'up',\n","  ',',\n","  'so',\n","  'he',\n","  'and',\n","  'tinker',\n","  'bell',\n","  'will',\n","  'go',\n","  'back',\n","  'to',\n","  'neverland',\n","  '.'],\n"," ['peter',\n","  'pan',\n","  'promised',\n","  'everyone',\n","  'that',\n","  'he',\n","  'will',\n","  'visit',\n","  'again',\n","  'sometime',\n","  '!'],\n"," ['and',\n","  'he',\n","  'flew',\n","  'out',\n","  'of',\n","  'the',\n","  'window',\n","  'with',\n","  'tinker',\n","  'bell',\n","  'by',\n","  'his',\n","  'side',\n","  '.']]"]},"metadata":{},"execution_count":11}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","import warnings \n","warnings.filterwarnings(action = 'ignore') \n","import gensim \n","from gensim.models import Word2Vec\n","  \n","sample = open(\"peter.txt\", \"r\", encoding='UTF8')\n","s = sample.read() \n","  \n","f = s.replace(\"\\n\", \" \")\n","data = [] \n","  \n","for i in sent_tokenize(f):\n","    temp = [] \n","    for j in word_tokenize(i):\n","        temp.append(j.lower())\n","    data.append(temp) \n","\n","data"]},{"cell_type":"code","execution_count":null,"id":"8789851f","metadata":{"id":"8789851f"},"outputs":[],"source":["#CBOW"]},{"cell_type":"code","execution_count":12,"id":"bcbd3aed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcbd3aed","outputId":"eec1ca60-4ade-4865-f157-97d3cf01c3df","executionInfo":{"status":"ok","timestamp":1642479013276,"user_tz":-540,"elapsed":261,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'peter' wendy' - CBOW :  1.0\n"]}],"source":["model1 = gensim.models.Word2Vec(data, min_count = 1,  \n","                              size = 100, window = 5, sg=0)    # 코랩에서는 vector_size를 사용하면 오류가 발생합니다. vector_size를 vector로 바꿔야 합니다.\n","print(\"Cosine similarity between 'peter' \" +\n","                 \"wendy' - CBOW : \", \n","      model1.wv.similarity('wendy', 'wendy'))\n","\n","model1 = gensim.models.Word2Vec(data, min_count = 1,  \n","                              size = 100, window = 5)"]},{"cell_type":"code","execution_count":13,"id":"9f6eac9a","metadata":{"id":"9f6eac9a","outputId":"8162706e-b145-4a7a-e310-538b609c2e2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642479015534,"user_tz":-540,"elapsed":256,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'peter' hook' - CBOW :  -0.017870199\n"]}],"source":["print(\"Cosine similarity between 'peter' \" +\n","                 \"hook' - CBOW : \", \n","      model1.wv.similarity('peter', 'hook')) "]},{"cell_type":"code","execution_count":null,"id":"08e2366f","metadata":{"id":"08e2366f"},"outputs":[],"source":["#Skip-gram"]},{"cell_type":"code","execution_count":14,"id":"4fb8edd9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fb8edd9","outputId":"12ae7e42-53e8-4158-cac5-ce8cae5aef1f","executionInfo":{"status":"ok","timestamp":1642479017285,"user_tz":-540,"elapsed":262,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'peter' wendy' - Skip Gram :  0.29419532\n"]}],"source":["model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n","                                             window = 5, sg = 1)\n","print(\"Cosine similarity between 'peter' \" +\n","          \"wendy' - Skip Gram : \", \n","    model2.wv.similarity('peter', 'wendy'))"]},{"cell_type":"code","execution_count":15,"id":"8128bcb6","metadata":{"id":"8128bcb6","outputId":"138258e9-5981-4267-9555-78e9d84225aa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642479020348,"user_tz":-540,"elapsed":279,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity between 'peter' hook' - Skip Gram :  0.45883453\n"]}],"source":["print(\"Cosine similarity between 'peter' \" +\n","            \"hook' - Skip Gram : \", \n","      model2.wv.similarity('peter', 'hook')) "]},{"cell_type":"code","execution_count":null,"id":"2f6e37fd","metadata":{"id":"2f6e37fd"},"outputs":[],"source":["#FastText\n","#FastText는 교재대로 진행하면 코랩에서 오류가 발생하여 새로운 코드를 삽입했습니다."]},{"cell_type":"code","source":["#코랩에서 교재와 같이 실행하면 다음과 같은 오류가 발생합니다.\n","#all ngrams for word peter absent from model\n","#어떤 ngram도 'perter', 'wendy'라는 단어가 존재하지 않는다는 의미인데, 단어가 존재함에도 찾지 못하여 예제를 조금 바꾸었습니다.\n","\n","import gensim\n","from gensim.models.fasttext import FastText as ft_gensim\n","stemmed = ['database', 'science', 'scientist', 'mgmt', 'microsoft', 'hire', 'develop', 'mentor', 'team', 'data', 'scientist', 'define', 'dataloader', 'scienc', 'priority', 'deep', 'understand', 'learn', 'goal', 'collabor', 'across', 'triple', 'group', 'set', 'team', 'shortterm', 'longterm', 'goal', 'act', 'strait', 'advisor', 'leadership', 'influenc', 'future', 'direct', 'strategy', 'define', 'partnership', 'align', 'effect', 'broad', 'analyt', 'effort', 'analyticsdata', 'team', 'drive', 'part', 'datadog', 'scienc', 'bi', 'common', 'disciplin', 'microsoftprior', 'experi', 'hire', 'manage', 'runner', 'team', 'data', 'scientist', 'busi', 'domain', 'experi', 'usage', 'analyt', 'must', 'experi', 'across', 'sever', 'relev', 'busi', 'domain', 'util', 'critic', 'think', 'skill', 'concept', 'complex', 'busi', 'problem', 'salt', 'use', 'advanc', 'analsis', 'large', 'scale', 'realworld', 'busi', 'data', 'set', 'candid', 'must', 'abl', 'independ', 'execut', 'analyt', 'project', 'help', 'intern', 'client', 'understand']\n","def gen_words(stemmed):\n","    yield stemmed   \n","\n","model = ft_gensim(size=100, window=5, min_count=1, workers=4, sg=1)\n","model.build_vocab(gen_words(stemmed))\n","\n","model.train(gen_words(stemmed), total_examples=model.corpus_count, epochs=model.iter)\n","#model.wv.similarity('data', 'scientist')\n","model.wv.most_similar(positive=['scientist'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV__Oeo6JskG","outputId":"840b4474-af80-4575-a9f4-cd885c34f47f","executionInfo":{"status":"ok","timestamp":1642479041404,"user_tz":-540,"elapsed":666,"user":{"displayName":"안윤경","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08326209251813559422"}}},"id":"lV__Oeo6JskG","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('scienc', 0.40221747756004333),\n"," ('science', 0.35087889432907104),\n"," ('must', 0.3002471923828125),\n"," ('problem', 0.23477956652641296),\n"," ('part', 0.22043737769126892),\n"," ('experi', 0.1733037531375885),\n"," ('data', 0.1570461541414261),\n"," ('bi', 0.12951719760894775),\n"," ('direct', 0.1187698021531105),\n"," ('broad', 0.10711653530597687)]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#https://fasttext.cc/docs/en/pretrained-vectors.html에서 wiki.ko.vec 파일을 따로 내려받으세요. \n","# 내려받은 파일을 불러와 실습합니다.\n","# 파일 크기가 약 2G 이상이므로 PC에서 파일을 불러오는 시간이 오래 걸리기 때문에 구글 드라이브에 파일을 넣어둔 후 불러오겠습니다.\n","# PC에서 파일을 불러오려면 아래 코드를 주석 해제 후 실습해주세요.\n","#from google.colab import files # 데이터 불러오기\n","#file_uploaded=files.upload()\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5pDEf67DOCU","outputId":"5632fcf7-e478-46ed-de96-ad2e5a678255"},"id":"l5pDEf67DOCU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":null,"id":"4543f02c","metadata":{"id":"4543f02c"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","\n","model_kr = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Colab Notebooks/wiki.ko.vec') #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","\n","#model_kr = KeyedVectors.load_word2vec_format('wiki.ko.vec')  #구글 드라이브가 아니라 PC에서 파일을 불러왔다면 주석 해제 후 실습"]},{"cell_type":"code","execution_count":null,"id":"b5fb5c1b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5fb5c1b","outputId":"bc9597b2-8f58-4551-dd9f-0cd3e178dc0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word: 노력함, Similarity: 0.80\n","Word: 노력중, Similarity: 0.75\n","Word: 노력만, Similarity: 0.72\n","Word: 노력과, Similarity: 0.71\n","Word: 노력의, Similarity: 0.69\n","Word: 노력가, Similarity: 0.69\n","Word: 노력이나, Similarity: 0.69\n","Word: 노력없이, Similarity: 0.68\n","Word: 노력맨, Similarity: 0.68\n","Word: 노력보다는, Similarity: 0.68\n"]}],"source":["find_similar_to = '노력'\n","\n","for similar_word in model_kr.similar_by_word(find_similar_to):\n","    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n","        similar_word[0], similar_word[1]\n","    ))"]},{"cell_type":"code","execution_count":null,"id":"346acbc3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"346acbc3","outputId":"2ac1364e-519f-4a4c-e10a-308542251aa9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('초식동물', 0.7804122567176819), ('거대동물', 0.7547270059585571), ('육식동물의', 0.7547166347503662), ('유두동물', 0.7535113096237183), ('반추동물', 0.7470757961273193), ('독동물', 0.7466292381286621), ('육상동물', 0.746031641960144), ('유즐동물', 0.7450904846191406), ('극피동물', 0.7449344992637634), ('복모동물', 0.7424346208572388)]\n"]}],"source":["similarities = model_kr.most_similar(positive=['동물', '육식동물'], negative=['사람'])\n","print(similarities)"]},{"cell_type":"code","execution_count":null,"id":"376e7c60","metadata":{"id":"376e7c60"},"outputs":[],"source":["#10.1.4 횟수/예측기반 임베딩\n","#GloVe"]},{"cell_type":"code","execution_count":null,"id":"c5752236","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5752236","outputId":"cf3f266d-9c13-48ed-8e9b-2844cc0c3026"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(400000, 100)"]},"metadata":{},"execution_count":9}],"source":["# PC에서 파일 불러오는 시간이 오래 걸리기 때문에 구글 드라이브에서 불러옵니다.\n","\n","#구글 드라이브가 아니라 PC에서 파일을 불러오려면 아래 주석 해제\n","#from google.colab import files # 데이터 불러오기\n","#file_uploaded=files.upload()\n","\n","import numpy as np\n","%matplotlib notebook\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","from sklearn.decomposition import PCA\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_file = datapath('/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt')  #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요. \n","#glove_file = datapath('glove.6B.100d.txt')                                        # PC에서 불러온다면 주석 해제 후 실습\n","word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n","glove2word2vec(glove_file, word2vec_glove_file)"]},{"cell_type":"code","execution_count":null,"id":"3db496ab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3db496ab","outputId":"0cd35e76-cb4b-4aba-cbe7-c71aa1857eda"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('legislation', 0.8072140216827393),\n"," ('proposal', 0.7306863069534302),\n"," ('senate', 0.7142540812492371),\n"," ('bills', 0.7044401168823242),\n"," ('measure', 0.6958035230636597),\n"," ('passed', 0.6906244158744812),\n"," ('amendment', 0.6846879720687866),\n"," ('provision', 0.6845567226409912),\n"," ('plan', 0.6816462874412537),\n"," ('clinton', 0.6663139462471008)]"]},"metadata":{},"execution_count":10}],"source":["model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n","model.most_similar('bill')"]},{"cell_type":"code","execution_count":null,"id":"57d14331","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57d14331","outputId":"7121034f-db39-439c-cee3-bd68f6e3377c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('peach', 0.688809871673584),\n"," ('mango', 0.6838189959526062),\n"," ('plum', 0.6684104204177856),\n"," ('berry', 0.6590359210968018),\n"," ('grove', 0.6581551432609558),\n"," ('blossom', 0.6503506302833557),\n"," ('raspberry', 0.6477391719818115),\n"," ('strawberry', 0.6442098617553711),\n"," ('pine', 0.6390928626060486),\n"," ('almond', 0.6379213333129883)]"]},"metadata":{},"execution_count":11}],"source":["model.most_similar('cherry') "]},{"cell_type":"code","execution_count":null,"id":"c609124b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c609124b","outputId":"6710226f-e10a-4587-c182-90bd5b9f4b6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('str94', 0.5899436473846436),\n"," ('http://www.ecb.int', 0.5723982453346252),\n"," ('rw95', 0.5641242265701294),\n"," ('js04bb', 0.5608091354370117),\n"," ('http://www.opel.com', 0.5586654543876648),\n"," ('obloquy', 0.5543686747550964),\n"," ('backstrap', 0.5506628155708313),\n"," ('disinfects', 0.5451074242591858),\n"," ('shepherdesses', 0.5444406270980835),\n"," ('hereros', 0.5441645383834839)]"]},"metadata":{},"execution_count":12}],"source":["model.most_similar(negative='cherry')"]},{"cell_type":"code","execution_count":null,"id":"21c9f828","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21c9f828","outputId":"6d557933-b3ba-48e0-bfc1-ee726fcdda69"},"outputs":[{"output_type":"stream","name":"stdout","text":["queen: 0.7699\n"]}],"source":["result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n","print(\"{}: {:.4f}\".format(*result[0]))"]},{"cell_type":"code","execution_count":null,"id":"72881522","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"72881522","outputId":"e151bc2e-369c-4312-e313-1442b56840ea"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'champagne'"]},"metadata":{},"execution_count":14}],"source":["def analogy(x1, x2, y1):\n","    result = model.most_similar(positive=[y1, x2], negative=[x1])\n","    return result[0][0]\n","analogy('australia', 'beer', 'france')"]},{"cell_type":"code","execution_count":null,"id":"9a8a76cf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9a8a76cf","outputId":"f266e544-4818-4c93-d881-9318ee64b682"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'longest'"]},"metadata":{},"execution_count":15}],"source":["analogy('tall', 'tallest', 'long')"]},{"cell_type":"code","execution_count":null,"id":"820bf3a4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"820bf3a4","outputId":"c89b9b1e-af28-40d3-c365-9c1eb92a2072"},"outputs":[{"output_type":"stream","name":"stdout","text":["cereal\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"]}],"source":["print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"]},{"cell_type":"code","execution_count":null,"id":"b1d9c854","metadata":{"id":"b1d9c854"},"outputs":[],"source":["#10.2 Transformer attention\n","#10.2.1 Seq2seq"]},{"cell_type":"code","execution_count":null,"id":"6ae5b052","metadata":{"id":"6ae5b052"},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","import re\n","import random\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"c4841b42","metadata":{"id":"c4841b42"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","MAX_LENGTH = 20\n","\n","class Lang:\n","    def __init__(self):\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  \n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"]},{"cell_type":"code","execution_count":null,"id":"58ebe3bd","metadata":{"id":"58ebe3bd"},"outputs":[],"source":["def normalizeString(df, lang):\n","    sentence = df[lang].str.lower()\n","    sentence = sentence.str.replace('[^A-Za-z\\s]+', '')\n","    sentence = sentence.str.normalize('NFD')\n","    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n","    return sentence\n","\n","def read_sentence(df, lang1, lang2):\n","    sentence1 = normalizeString(df, lang1)\n","    sentence2 = normalizeString(df, lang2)\n","    return sentence1, sentence2\n","\n","def read_file(loc, lang1, lang2):\n","    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n","    return df\n","\n","def process_data(lang1,lang2):\n","    df = read_file('/content/drive/MyDrive/Colab Notebooks/eng-fra.txt', lang1, lang2) #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n","\n","    input_lang = Lang()\n","    output_lang = Lang()\n","    pairs = []\n","    for i in range(len(df)):\n","        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n","            full = [sentence1[i], sentence2[i]]\n","            input_lang.addSentence(sentence1[i])\n","            output_lang.addSentence(sentence2[i])\n","            pairs.append(full)\n","\n","    return input_lang, output_lang, pairs"]},{"cell_type":"code","execution_count":null,"id":"f0c3e7a2","metadata":{"id":"f0c3e7a2"},"outputs":[],"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorsFromPair(input_lang, output_lang, pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"]},{"cell_type":"code","execution_count":null,"id":"e5cda608","metadata":{"id":"e5cda608"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n","        super(Encoder, self).__init__()       \n","        self.input_dim = input_dim\n","        self.embbed_dim = embbed_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n","        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n","              \n","    def forward(self, src):      \n","        embedded = self.embedding(src).view(1,1,-1)\n","        outputs, hidden = self.gru(embedded)\n","        return outputs, hidden"]},{"cell_type":"code","execution_count":null,"id":"8c8c263c","metadata":{"id":"8c8c263c"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n","        super(Decoder, self).__init__()\n","\n","        self.embbed_dim = embbed_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n","        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n","        self.out = nn.Linear(self.hidden_dim, output_dim)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","      \n","    def forward(self, input, hidden):\n","        input = input.view(1, -1)\n","        embedded = F.relu(self.embedding(input))\n","        output, hidden = self.gru(embedded, hidden)       \n","        prediction = self.softmax(self.out(output[0]))      \n","        return prediction, hidden"]},{"cell_type":"code","execution_count":null,"id":"2e32442e","metadata":{"id":"2e32442e"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n","        super().__init__()\n","      \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","     \n","    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n","\n","        input_length = input_lang.size(0)\n","        batch_size = output_lang.shape[1] \n","        target_length = output_lang.shape[0]\n","        vocab_size = self.decoder.output_dim      \n","        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n","\n","        for i in range(input_length):\n","            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n","\n","        decoder_hidden = encoder_hidden.to(device)  \n","        decoder_input = torch.tensor([SOS_token], device=device)  \n","\n","        for t in range(target_length):   \n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            outputs[t] = decoder_output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            topv, topi = decoder_output.topk(1)\n","            input = (output_lang[t] if teacher_force else topi)\n","            if(teacher_force == False and input.item() == EOS_token):\n","                break\n","        return outputs"]},{"cell_type":"code","execution_count":null,"id":"93d4ebc3","metadata":{"id":"93d4ebc3"},"outputs":[],"source":["teacher_forcing_ratio = 0.5\n","\n","def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n","    model_optimizer.zero_grad()\n","    input_length = input_tensor.size(0)\n","    loss = 0\n","    epoch_loss = 0\n","    output = model(input_tensor, target_tensor)\n","    num_iter = output.size(0)\n","\n","    for ot in range(num_iter):\n","        loss += criterion(output[ot], target_tensor[ot])\n","\n","    loss.backward()\n","    model_optimizer.step()\n","    epoch_loss = loss.item() / num_iter\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"id":"9768b4e8","metadata":{"id":"9768b4e8"},"outputs":[],"source":["def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n","    model.train()\n","    optimizer = optim.SGD(model.parameters(), lr=0.01)\n","    criterion = nn.NLLLoss()\n","    total_loss_iterations = 0\n","\n","    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n","                      for i in range(num_iteration)]\n","  \n","    for iter in range(1, num_iteration+1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n","        total_loss_iterations += loss\n","\n","        if iter % 5000 == 0:\n","            avarage_loss= total_loss_iterations / 5000\n","            total_loss_iterations = 0\n","            print('%d %.4f' % (iter, avarage_loss))\n","          \n","    torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/mytraining.pt') \n","    return model"]},{"cell_type":"code","execution_count":null,"id":"9bd2079f","metadata":{"id":"9bd2079f"},"outputs":[],"source":["def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentences[0])\n","        output_tensor = tensorFromSentence(output_lang, sentences[1])  \n","        decoded_words = []  \n","        output = model(input_tensor, output_tensor)\n","  \n","        for ot in range(output.size(0)):\n","            topv, topi = output[ot].topk(1)\n","\n","            if topi[0].item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi[0].item()])\n","    return decoded_words\n","\n","def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('input {}'.format(pair[0]))\n","        print('output {}'.format(pair[1]))\n","        output_words = evaluate(model, input_lang, output_lang, pair)\n","        output_sentence = ' '.join(output_words)\n","        print('predicted {}'.format(output_sentence))"]},{"cell_type":"code","execution_count":null,"id":"b6814115","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6814115","outputId":"e544a926-669a-4f7b-b39c-855352af5e97"},"outputs":[{"output_type":"stream","name":"stdout","text":["random sentence ['i dont handle ultimatums well', 'je ne gre pas bien les ultimatums']\n","Input : 13366 Output : 25937\n","Encoder(\n","  (embedding): Embedding(13366, 256)\n","  (gru): GRU(256, 512)\n",")\n","Decoder(\n","  (embedding): Embedding(25937, 256)\n","  (gru): GRU(256, 512)\n","  (out): Linear(in_features=512, out_features=25937, bias=True)\n","  (softmax): LogSoftmax(dim=1)\n",")\n","5000 4.8850\n","10000 4.6709\n","15000 4.6027\n","20000 4.5851\n","25000 4.5963\n","30000 4.5049\n","35000 4.5060\n","40000 4.5039\n","45000 4.5135\n","50000 4.4676\n","55000 4.4547\n","60000 4.5054\n","65000 4.4584\n","70000 4.4132\n","75000 4.4566\n"]}],"source":["lang1 = 'eng'\n","lang2 = 'fra'\n","input_lang, output_lang, pairs = process_data(lang1, lang2)\n","\n","randomize = random.choice(pairs)\n","print('random sentence {}'.format(randomize))\n","\n","input_size = input_lang.n_words\n","output_size = output_lang.n_words\n","print('Input : {} Output : {}'.format(input_size, output_size))\n","\n","embed_size = 256\n","hidden_size = 512\n","num_layers = 1\n","num_iteration = 75000\n","\n","encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n","decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n","\n","model = Seq2Seq(encoder, decoder, device).to(device)\n"," \n","print(encoder)\n","print(decoder)\n","\n","model = trainModel(model, input_lang, output_lang, pairs, num_iteration)"]},{"cell_type":"code","execution_count":null,"id":"fa2807a7","metadata":{"id":"fa2807a7","outputId":"a85d3b36-7adb-4b23-8bb0-f8c280ff60e4","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["input can i get you anything else\n","output puisje vous procurer quoi que ce soit dautre \n","predicted je le pas le <EOS>\n","input she is indeed a lovely girl\n","output cest vraiment une fille adorable\n","predicted je le pas le <EOS>\n","input its about time you got a haircut\n","output il serait temps que tu te coupes les cheveux\n","predicted je le pas le <EOS>\n","input poyang lake is the largest freshwater lake in china\n","output le lac poyang est le plus grand lac deau douce de chine\n","predicted je le pas le <EOS>\n","input you look surprised\n","output vous avez lair surpris\n","predicted je le pas le <EOS>\n","input in hindsight this was a mistake\n","output avec le recul ctait une erreur\n","predicted je le pas le <EOS>\n","input it could be a trap\n","output cela pourrait tre un pige\n","predicted je le pas le <EOS>\n","input shall we walk or drive\n","output nous y allons  pied ou en voiture\n","predicted je le pas le <EOS>\n","input i feel like throwing up\n","output jai envie de dgueuler\n","predicted je le pas le <EOS>\n","input youre adorable\n","output vous tes adorables\n","predicted je le pas le\n"]}],"source":["evaluateRandomly(model, input_lang, output_lang, pairs)"]},{"cell_type":"code","execution_count":null,"id":"6032888a","metadata":{"id":"6032888a"},"outputs":[],"source":["class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights"]},{"cell_type":"code","execution_count":null,"id":"9bb7ef62","metadata":{"id":"9bb7ef62"},"outputs":[],"source":["def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  \n","    plot_loss_total = 0  \n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n","                      for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","        loss = Model(model, input_tensor, target_tensor, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % 5000 == 0:\n","            print_loss_avg = print_loss_total / 5000\n","            print_loss_total = 0\n","            print('%d,  %.4f' % (iter, print_loss_avg))"]},{"cell_type":"code","execution_count":null,"id":"6df879d6","metadata":{"id":"6df879d6","outputId":"ae91acb5-132e-4a11-c165-4ed912c07e08","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder(\n","  (embedding): Embedding(13366, 256)\n","  (gru): GRU(256, 512)\n",")\n","AttnDecoderRNN(\n","  (embedding): Embedding(25937, 512)\n","  (attn): Linear(in_features=1024, out_features=20, bias=True)\n","  (attn_combine): Linear(in_features=1024, out_features=512, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (gru): GRU(512, 512)\n","  (out): Linear(in_features=512, out_features=25937, bias=True)\n",")\n","5000,  4.7605\n","10000,  4.7334\n","15000,  4.7214\n","20000,  4.7251\n","25000,  4.7354\n","30000,  4.7388\n","35000,  4.7506\n","40000,  4.7378\n","45000,  4.7719\n","50000,  4.7333\n","55000,  4.7437\n","60000,  4.7430\n","65000,  4.7109\n","70000,  4.7223\n","75000,  4.7535\n"]}],"source":["import time\n","\n","embed_size = 256\n","hidden_size = 512\n","num_layers = 1\n","input_size = input_lang.n_words\n","output_size = output_lang.n_words\n","\n","encoder1 = Encoder(input_size, hidden_size, embed_size, num_layers)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n","\n","print(encoder1)\n","print(attn_decoder1)\n","\n","attn_model = trainIters(encoder1, attn_decoder1, 75000, print_every=5000, plot_every=100, learning_rate=0.01)"]},{"cell_type":"code","execution_count":null,"id":"201fd8b6","metadata":{"id":"201fd8b6"},"outputs":[],"source":["#10.2.1 Bert"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GalDYeCfJLJF","outputId":"f79abaab-72db-4113-a5d5-73648e4179a6"},"id":"GalDYeCfJLJF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 75.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 71.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 649 kB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"]}]},{"cell_type":"code","source":["!pip install pytorch-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DIsA0wggJZHI","outputId":"75eadc2c-080a-4f26-8b51-53d0344a0222"},"id":"DIsA0wggJZHI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.10.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n","Collecting boto3\n","  Downloading boto3-1.20.26-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 72.8 MB/s \n","\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.46)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.10.0.2)\n","Collecting botocore<1.24.0,>=1.23.26\n","  Downloading botocore-1.23.26-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 49.3 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.26->boto3->pytorch-transformers) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.26->boto3->pytorch-transformers) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 63.3 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.26 botocore-1.23.26 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sentencepiece-0.1.96 urllib3-1.25.11\n"]}]},{"cell_type":"code","execution_count":null,"id":"20b91e5b","metadata":{"id":"20b91e5b"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"0fa39b29","metadata":{"id":"0fa39b29"},"outputs":[],"source":["train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/training.txt', sep='\\t') #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","valid_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/validing.txt', sep='\\t') #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/testing.txt', sep='\\t') #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요."]},{"cell_type":"code","execution_count":null,"id":"e620c35f","metadata":{"id":"e620c35f"},"outputs":[],"source":["train_df = train_df.sample(frac=0.1, random_state=500)\n","valid_df = valid_df.sample(frac=0.1, random_state=500)\n","test_df = test_df.sample(frac=0.1, random_state=500)"]},{"cell_type":"code","execution_count":null,"id":"8506022e","metadata":{"id":"8506022e"},"outputs":[],"source":["class Datasets(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        text = self.df.iloc[idx, 1]\n","        label = self.df.iloc[idx, 2]\n","        return text, label"]},{"cell_type":"code","execution_count":null,"id":"97277c2b","metadata":{"id":"97277c2b"},"outputs":[],"source":["train_dataset = Datasets(train_df)\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n","\n","valid_dataset = Datasets(valid_df)\n","valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)\n","\n","test_dataset = Datasets(test_df)\n","test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=0)"]},{"cell_type":"code","execution_count":null,"id":"106307e9","metadata":{"id":"106307e9","outputId":"ae83bed4-2fea-439b-d654-67e8a67bdf5b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 2088267.02B/s]\n","100%|██████████| 433/433 [00:00<00:00, 155304.74B/s]\n","100%|██████████| 440473133/440473133 [00:07<00:00, 59240479.79B/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":31}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"id":"7c0f5fef","metadata":{"id":"7c0f5fef"},"outputs":[],"source":["def save_checkpoint(save_path, model, valid_loss):\n","    if save_path == None:\n","        return    \n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'valid_loss': valid_loss}\n","    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","def load_checkpoint(load_path, model):    \n","    if load_path==None:\n","        return    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')\n","    \n","    model.load_state_dict(state_dict['model_state_dict'])\n","    return state_dict['valid_loss']\n","\n","def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n","    if save_path == None:\n","        return    \n","    state_dict = {'train_loss_list': train_loss_list,\n","                  'valid_loss_list': valid_loss_list,\n","                  'global_steps_list': global_steps_list}    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","def load_metrics(load_path):\n","    if load_path==None:\n","        return    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')    \n","    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"]},{"cell_type":"code","execution_count":null,"id":"0871e661","metadata":{"id":"0871e661"},"outputs":[],"source":["def train(model,\n","          optimizer,\n","          criterion = nn.BCELoss(),\n","          num_epochs = 5,\n","          eval_every = len(train_loader) // 2,\n","          best_valid_loss = float(\"Inf\")):\n","    \n","    total_correct = 0.0\n","    total_len = 0.0\n","    running_loss = 0.0\n","    valid_running_loss = 0.0\n","    global_step = 0\n","    train_loss_list = []\n","    valid_loss_list = []\n","    global_steps_list = []\n","\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for text, label in train_loader:\n","            optimizer.zero_grad()        \n","            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n","        \n","            sample = torch.tensor(padded_list)\n","            sample, label = sample.to(device), label.to(device)\n","            labels = torch.tensor(label)\n","            outputs = model(sample, labels=labels)\n","            loss, logits = outputs\n","\n","            pred = torch.argmax(F.softmax(logits), dim=1)\n","            correct = pred.eq(labels)\n","            total_correct += correct.sum().item()\n","            total_len += len(labels)\n","            running_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()        \n","            global_step += 1\n","\n","            if global_step % eval_every == 0:\n","                model.eval()\n","                with torch.no_grad():                    \n","                    for text, label in valid_loader:\n","                        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","                        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]        \n","                        sample = torch.tensor(padded_list)\n","                        sample, label = sample.to(device), label.to(device)\n","                        labels = torch.tensor(label)\n","                        outputs = model(sample, labels=labels)\n","                        loss, logits = outputs                        \n","                        valid_running_loss += loss.item()\n","\n","                average_train_loss = running_loss / eval_every\n","                average_valid_loss = valid_running_loss / len(valid_loader)\n","                train_loss_list.append(average_train_loss)\n","                valid_loss_list.append(average_valid_loss)\n","                global_steps_list.append(global_step)\n","\n","                running_loss = 0.0                \n","                valid_running_loss = 0.0\n","                model.train()\n","\n","                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n","                              average_train_loss, average_valid_loss))\n","                \n","                if best_valid_loss > average_valid_loss:\n","                    best_valid_loss = average_valid_loss\n","                    save_checkpoint('/content/drive/MyDrive/Colab Notebooks/model.pt', model, best_valid_loss)\n","                    save_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    \n","    save_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    print('훈련 종료!')"]},{"cell_type":"code","execution_count":null,"id":"7f69dac7","metadata":{"id":"7f69dac7","outputId":"8b91230a-a285-4d39-8803-bd0e73fc8691","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Step [510/5100], Train Loss: 0.7034, Valid Loss: 0.6940\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/model.pt\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/metrics.pt\n","Epoch [1/5], Step [1020/5100], Train Loss: 0.6986, Valid Loss: 0.7357\n","Epoch [2/5], Step [1530/5100], Train Loss: 0.7006, Valid Loss: 0.7012\n","Epoch [2/5], Step [2040/5100], Train Loss: 0.7062, Valid Loss: 0.6969\n","Epoch [3/5], Step [2550/5100], Train Loss: 0.7103, Valid Loss: 0.7006\n","Epoch [3/5], Step [3060/5100], Train Loss: 0.7024, Valid Loss: 0.6927\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/model.pt\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/metrics.pt\n","Epoch [4/5], Step [3570/5100], Train Loss: 0.7079, Valid Loss: 0.6949\n","Epoch [4/5], Step [4080/5100], Train Loss: 0.7092, Valid Loss: 0.6974\n","Epoch [5/5], Step [4590/5100], Train Loss: 0.6981, Valid Loss: 0.7013\n","Epoch [5/5], Step [5100/5100], Train Loss: 0.7000, Valid Loss: 0.6955\n","Model saved to ==> /content/drive/MyDrive/Colab Notebooks/metrics.pt\n","훈련 종료!\n"]}],"source":["optimizer = optim.Adam(model.parameters(), lr=2e-5)\n","train(model=model, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"id":"2abbb6cb","metadata":{"id":"2abbb6cb","outputId":"010e8dbc-4540-4465-abd0-16c112ebc3b6","colab":{"base_uri":"https://localhost:8080/","height":297}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/metrics.pt\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["train_loss_list, valid_loss_list, global_steps_list = load_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt')\n","plt.plot(global_steps_list, train_loss_list, label='Train')\n","plt.plot(global_steps_list, valid_loss_list, label='Valid')\n","plt.xlabel('Global Steps')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show() "]},{"cell_type":"code","execution_count":null,"id":"0cb68692","metadata":{"id":"0cb68692"},"outputs":[],"source":["def evaluate(model, test_loader):\n","    y_pred = []\n","    y_true = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for text, label in test_loader:\n","            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n","        \n","            sample = torch.tensor(padded_list)\n","            sample, label = sample.to(device), label.to(device)\n","            labels = torch.tensor(label)\n","            output = model(sample, labels=labels)\n","            \n","            _, output = output\n","            y_pred.extend(torch.argmax(output, 1).tolist())\n","            y_true.extend(labels.tolist())\n","                    \n","    print('Classification 결과:')\n","    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n","    \n","    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n","    ax= plt.subplot()\n","    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n","    \n","    ax.set_title('Confusion Matrix')\n","    ax.set_xlabel('Predicted Labels')\n","    ax.set_ylabel('True Labels')\n","    ax.xaxis.set_ticklabels(['0', '1'])\n","    ax.yaxis.set_ticklabels(['0', '1'])"]},{"cell_type":"code","execution_count":null,"id":"97dfe435","metadata":{"id":"97dfe435","outputId":"cf1e6ea3-6ca0-4ad0-cd73-5e1a2b0c8e26","colab":{"base_uri":"https://localhost:8080/","height":494}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded from <== /content/drive/MyDrive/Colab Notebooks/model.pt\n","Classification 결과:\n","              precision    recall  f1-score   support\n","\n","           1     0.5091    1.0000    0.6747       558\n","           0     0.0000    0.0000    0.0000       538\n","\n","    accuracy                         0.5091      1096\n","   macro avg     0.2546    0.5000    0.3374      1096\n","weighted avg     0.2592    0.5091    0.3435      1096\n","\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["import warnings\n","warnings.filterwarnings('ignore') \n","best_model = model.to(device)\n","load_checkpoint('/content/drive/MyDrive/Colab Notebooks/model.pt', best_model)\n","evaluate(best_model, test_loader)"]},{"cell_type":"code","execution_count":null,"id":"c64b382c","metadata":{"id":"c64b382c"},"outputs":[],"source":["#10.3 한국어 임베딩"]},{"cell_type":"code","execution_count":null,"id":"e14558a2","metadata":{"colab":{"referenced_widgets":["20d1c612925c4645a4f082cebe783877","52a10b41f72347458716d5cb98b9dc3b","4d5b6242b45e419fbfc9aaf3f0ce00b6","5b7669e6a48f43cfa1ac5b3b11f3535f","8a94f31a63834892850bf5bed30d1c3d","32ff01612e954a528bb92939f370bd43","37a099645162466c866035e6f23fee86","77cbc7c4e57d471da5af67e1f6b85921","1ec11563f0f54df49afb15cfb4e86fca","deb21092fcb349bc89da18d63d1a43d4","d725e35c9a554dfbbc5c077040d69922","a7173ba4ac5644498bf4b006e1e2be1c","0c24d419c45b463980b15d620a601d44","3415e96eea0144679125531f6c817223","8cbe4b8d19814ee5ac1a2a9eb33d6c77","e64802949a6a4a119860f8a31deb1ef0","dc8ed9fe36cc4904853b26ca5d37d18d","d545e99ffcc248e9ac90746e3bb27342","8728eb50de1a486dbe81fcaecd11790b","248f5471430e475b9da952b69446a07e","390fef538dc0400890fe85d559d916d4","7e16df4f753b4449b796742ec0b7f4d8","c71d891c1be54b579f7f85f3165ddee1","4a35b1db973743c2919ac2ed9ed878d2","c4721e93bf1a44bd9548a4bdfe18415b","68d9ca52f5964bee82d39a7382354190","7631d6b4cd8b43c5b16bd7645c5b6224","d6c6070eca9741a7be4ccc739fc9e4a4","32244af61f3644f09d1579b052a7ec3b","2e39562c83fc4fdd9ae5cf55d1e041f9","22b9fd542cd24d59bae3c19c143437d9","e6ee8921178847e08b9ad0854cd7753f","fbbaabe2699a452089503525f9c34c9a","2acb8fa61fab4ea7a98fa87340fa075a","8e04eca8c9bb437fac125fd871aa93fa","9b1835e57a0e4f5eaef731235f54b353","9ddcf6529cbd4126bc11367107fd979a","2421c01201f14a19858bab34d52dde00","6cdfe1814158464c86a04d467a70dd80","4757384a8cee42ac98bcf5b4c035b0e7","937f2611234e479ea779bf96bb8f01c7","fa291becb0584820ae00c4dedcb2919f","e693fcf6cacd48a080f6a9cfc67464e3","621ec3144103493896fe077668542a78"],"base_uri":"https://localhost:8080/","height":145},"id":"e14558a2","outputId":"726184a1-31b9-47d1-b1ce-cf5c07e216ea"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20d1c612925c4645a4f082cebe783877","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7173ba4ac5644498bf4b006e1e2be1c","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c71d891c1be54b579f7f85f3165ddee1","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2acb8fa61fab4ea7a98fa87340fa075a","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"]},"metadata":{}}],"source":["import torch\n","from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"]},{"cell_type":"code","execution_count":null,"id":"cfe6d8ae","metadata":{"id":"cfe6d8ae","outputId":"a0125953-724a-46f1-aa05-c8af904fff81","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', '나는', '파', '##이', '##토', '##치를', '이', '##용한', '딥', '##러', '##닝', '##을', '학', '##습', '##중', '##이다', '.', '[SEP]']\n"]}],"source":["text = \"나는 파이토치를 이용한 딥러닝을 학습중이다.\"\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","tokenized_text = tokenizer.tokenize(marked_text)\n","print(tokenized_text)"]},{"cell_type":"code","execution_count":null,"id":"3740f31f","metadata":{"id":"3740f31f","outputId":"84807e79-1ed3-42ea-b454-1da3c0a8e64f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]           101\n","과             8,898\n","##수          15,891\n","##원에         108,280\n","사             9,405\n","##과          11,882\n","##가          11,287\n","많             9,249\n","##았다         27,303\n",".               119\n","친             9,781\n","##구          17,196\n","##가          11,287\n","나             8,982\n","##에게         26,212\n","사             9,405\n","##과          11,882\n","##했다         12,490\n",".               119\n","백             9,331\n","##설          31,928\n","##공          28,000\n","##주는         100,633\n","독             9,088\n","##이          10,739\n","든             9,115\n","사             9,405\n","##과          11,882\n","##를          11,513\n","먹             9,266\n","##었다         17,706\n",".               119\n","[SEP]           102\n"]}],"source":["text = \"과수원에 사과가 많았다.\" \\\n","       \"친구가 나에게 사과했다.\"\\\n","       \"백설공주는 독이 든 사과를 먹었다.\"\n","\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","tokenized_text = tokenizer.tokenize(marked_text)\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","for tup in zip(tokenized_text, indexed_tokens):\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"]},{"cell_type":"code","execution_count":null,"id":"9da9f047","metadata":{"id":"9da9f047","outputId":"e9a14b02-fa35-40ec-ce4d-fe30f239a612","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["segments_ids = [1] * len(tokenized_text)\n","print (segments_ids)"]},{"cell_type":"code","execution_count":null,"id":"1b9a81c0","metadata":{"id":"1b9a81c0"},"outputs":[],"source":["tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])"]},{"cell_type":"code","execution_count":null,"id":"2637806e","metadata":{"colab":{"referenced_widgets":["9a1429d0febe4fcd92f12643336bf543","8c9f286bfdf045de8760ac8be20909bd","0e359a7528e74e069ed7e54ab7fe67a3","34a7130461e84e8580ab3ad337515b6e","accfc711ae884645a2f9c151aeb8df6d","d2f60f37dc7f482e88713631caa8cb8a","dd345b306bce47f9a5319001ce987e1d","e21e6890730a4b57bebb77fd865ef980","337801181d8f4dc3bc7705439cfeb656","4d4d590de38c4c20a13441785f87dfe0","37c0c430e3dd452b9dea5ebfaf1d46ad"],"base_uri":"https://localhost:8080/","height":1000},"id":"2637806e","outputId":"20c6680e-1063-4d59-cb4b-cfc13e50fdb7"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a1429d0febe4fcd92f12643336bf543","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":45}],"source":["model = BertModel.from_pretrained('bert-base-multilingual-cased',\n","                                  output_hidden_states = True,)\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"id":"3ba79d70","metadata":{"id":"3ba79d70"},"outputs":[],"source":["with torch.no_grad():\n","    outputs = model(tokens_tensor, segments_tensors)\n","    hidden_states = outputs[2]"]},{"cell_type":"code","execution_count":null,"id":"e61815e4","metadata":{"id":"e61815e4","outputId":"774f049b-cb84-414c-d6e5-3fafae322018","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["계층 수: 13   (initial embeddings + 12 BERT layers)\n","배치 수: 1\n","토큰 수: 33\n","은닉층 유닛 수: 768\n"]}],"source":["print (\"계층 수:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n","layer_i = 0\n","\n","print (\"배치 수:\", len(hidden_states[layer_i]))\n","batch_i = 0\n","\n","print (\"토큰 수:\", len(hidden_states[layer_i][batch_i]))\n","token_i = 0\n","\n","print (\"은닉층 유닛 수:\", len(hidden_states[layer_i][batch_i][token_i]))"]},{"cell_type":"code","execution_count":null,"id":"6c79e83e","metadata":{"id":"6c79e83e","outputId":"8222224a-7da8-4e04-a10e-e9f6be6c60e6","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["은닉 상태의 유형:  <class 'tuple'>\n","각 계층에서의 텐서 형태:  torch.Size([1, 33, 768])\n"]}],"source":["print('은닉 상태의 유형: ', type(hidden_states))\n","print('각 계층에서의 텐서 형태: ', hidden_states[0].size())"]},{"cell_type":"code","execution_count":null,"id":"9633fe69","metadata":{"id":"9633fe69","outputId":"5d45ce34-149b-4077-d2b1-4c2d5a83b094","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 1, 33, 768])"]},"metadata":{},"execution_count":49}],"source":["token_embeddings = torch.stack(hidden_states, dim=0)\n","token_embeddings.size()"]},{"cell_type":"code","execution_count":null,"id":"5d777453","metadata":{"id":"5d777453","outputId":"a16c7d6d-bcf6-465a-bfc2-429c4f7b1fdb","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 33, 768])"]},"metadata":{},"execution_count":50}],"source":["token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","token_embeddings.size()"]},{"cell_type":"code","execution_count":null,"id":"aa6a4d37","metadata":{"id":"aa6a4d37","outputId":"623aff8c-042c-4f91-d88a-b3aae7fbafa8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([33, 13, 768])"]},"metadata":{},"execution_count":51}],"source":["token_embeddings = token_embeddings.permute(1,0,2)\n","token_embeddings.size()"]},{"cell_type":"code","execution_count":null,"id":"5b2f7874","metadata":{"id":"5b2f7874","outputId":"740d657e-6388-4362-f2c3-f6eb9e61d75d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["형태는: 33 x 3072\n"]}],"source":["token_vecs_cat = []\n","for token in token_embeddings:\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n","    token_vecs_cat.append(cat_vec)\n","print ('형태는: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"]},{"cell_type":"code","execution_count":null,"id":"7621bc1b","metadata":{"id":"7621bc1b","outputId":"a32ab126-707f-419f-a994-0df89870a1ba","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["형태는: 33 x 768\n"]}],"source":["token_vecs_sum = []\n","for token in token_embeddings:\n","    sum_vec = torch.sum(token[-4:], dim=0)\n","    token_vecs_sum.append(sum_vec)\n","print ('형태는: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"]},{"cell_type":"code","execution_count":null,"id":"2a14ae7e","metadata":{"id":"2a14ae7e","outputId":"e9ad5635-b53f-4aad-a038-e8c8a88daa5c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["최종 임베딩 벡터의 형태: torch.Size([768])\n"]}],"source":["token_vecs = hidden_states[-2][0]\n","sentence_embedding = torch.mean(token_vecs, dim=0)\n","print (\"최종 임베딩 벡터의 형태:\", sentence_embedding.size())"]},{"cell_type":"code","execution_count":null,"id":"a0b581d9","metadata":{"id":"a0b581d9","outputId":"214f0aec-a66f-4c1c-bac0-59844bb946eb","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["0 [CLS]\n","1 과\n","2 ##수\n","3 ##원에\n","4 사\n","5 ##과\n","6 ##가\n","7 많\n","8 ##았다\n","9 .\n","10 친\n","11 ##구\n","12 ##가\n","13 나\n","14 ##에게\n","15 사\n","16 ##과\n","17 ##했다\n","18 .\n","19 백\n","20 ##설\n","21 ##공\n","22 ##주는\n","23 독\n","24 ##이\n","25 든\n","26 사\n","27 ##과\n","28 ##를\n","29 먹\n","30 ##었다\n","31 .\n","32 [SEP]\n"]}],"source":["for i, token_str in enumerate(tokenized_text):\n","    print (i, token_str)"]},{"cell_type":"code","execution_count":null,"id":"eb56c9a3","metadata":{"id":"eb56c9a3","outputId":"77a957f0-c697-4e3c-d81c-7a4a04d9569b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["사과가 많았다 tensor([-0.5844, -4.0836,  0.4906,  0.8915, -1.8054])\n","나에게 사과했다 tensor([-0.8631, -3.4047, -0.7351,  0.9805, -2.6700])\n","사과를 먹었다 tensor([ 0.6756, -0.3618,  0.0586,  2.2050, -2.4193])\n"]}],"source":["print(\"사과가 많았다\", str(token_vecs_sum[6][:5]))\n","print(\"나에게 사과했다\", str(token_vecs_sum[10][:5]))\n","print(\"사과를 먹었다\", str(token_vecs_sum[19][:5]))"]},{"cell_type":"code","execution_count":null,"id":"dbc2e65c","metadata":{"id":"dbc2e65c","outputId":"a1b5b189-afda-427d-d61a-be0142cd2c94","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["*유사한* 의미에 대한 벡터 유사성:  0.86\n","*다른* 의미에 대한 벡터 유사성:  0.91\n"]}],"source":["from scipy.spatial.distance import cosine\n","diff_apple = 1 - cosine(token_vecs_sum[5], token_vecs_sum[27])\n","same_apple = 1 - cosine(token_vecs_sum[5], token_vecs_sum[16])\n","print('*유사한* 의미에 대한 벡터 유사성:  %.2f' % same_apple)\n","print('*다른* 의미에 대한 벡터 유사성:  %.2f' % diff_apple)"]},{"cell_type":"code","execution_count":null,"id":"7c1c53aa","metadata":{"id":"7c1c53aa"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"colab_10문의.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"20d1c612925c4645a4f082cebe783877":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_52a10b41f72347458716d5cb98b9dc3b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d5b6242b45e419fbfc9aaf3f0ce00b6","IPY_MODEL_5b7669e6a48f43cfa1ac5b3b11f3535f","IPY_MODEL_8a94f31a63834892850bf5bed30d1c3d"]}},"52a10b41f72347458716d5cb98b9dc3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d5b6242b45e419fbfc9aaf3f0ce00b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_32ff01612e954a528bb92939f370bd43","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_37a099645162466c866035e6f23fee86"}},"5b7669e6a48f43cfa1ac5b3b11f3535f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_77cbc7c4e57d471da5af67e1f6b85921","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ec11563f0f54df49afb15cfb4e86fca"}},"8a94f31a63834892850bf5bed30d1c3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_deb21092fcb349bc89da18d63d1a43d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 972k/972k [00:00&lt;00:00, 1.64MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d725e35c9a554dfbbc5c077040d69922"}},"32ff01612e954a528bb92939f370bd43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"37a099645162466c866035e6f23fee86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77cbc7c4e57d471da5af67e1f6b85921":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1ec11563f0f54df49afb15cfb4e86fca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"deb21092fcb349bc89da18d63d1a43d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d725e35c9a554dfbbc5c077040d69922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7173ba4ac5644498bf4b006e1e2be1c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0c24d419c45b463980b15d620a601d44","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3415e96eea0144679125531f6c817223","IPY_MODEL_8cbe4b8d19814ee5ac1a2a9eb33d6c77","IPY_MODEL_e64802949a6a4a119860f8a31deb1ef0"]}},"0c24d419c45b463980b15d620a601d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3415e96eea0144679125531f6c817223":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dc8ed9fe36cc4904853b26ca5d37d18d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d545e99ffcc248e9ac90746e3bb27342"}},"8cbe4b8d19814ee5ac1a2a9eb33d6c77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8728eb50de1a486dbe81fcaecd11790b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_248f5471430e475b9da952b69446a07e"}},"e64802949a6a4a119860f8a31deb1ef0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_390fef538dc0400890fe85d559d916d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 879B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e16df4f753b4449b796742ec0b7f4d8"}},"dc8ed9fe36cc4904853b26ca5d37d18d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d545e99ffcc248e9ac90746e3bb27342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8728eb50de1a486dbe81fcaecd11790b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"248f5471430e475b9da952b69446a07e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"390fef538dc0400890fe85d559d916d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7e16df4f753b4449b796742ec0b7f4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c71d891c1be54b579f7f85f3165ddee1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4a35b1db973743c2919ac2ed9ed878d2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c4721e93bf1a44bd9548a4bdfe18415b","IPY_MODEL_68d9ca52f5964bee82d39a7382354190","IPY_MODEL_7631d6b4cd8b43c5b16bd7645c5b6224"]}},"4a35b1db973743c2919ac2ed9ed878d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4721e93bf1a44bd9548a4bdfe18415b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d6c6070eca9741a7be4ccc739fc9e4a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_32244af61f3644f09d1579b052a7ec3b"}},"68d9ca52f5964bee82d39a7382354190":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2e39562c83fc4fdd9ae5cf55d1e041f9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1961828,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1961828,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_22b9fd542cd24d59bae3c19c143437d9"}},"7631d6b4cd8b43c5b16bd7645c5b6224":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e6ee8921178847e08b9ad0854cd7753f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.87M/1.87M [00:00&lt;00:00, 1.59MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fbbaabe2699a452089503525f9c34c9a"}},"d6c6070eca9741a7be4ccc739fc9e4a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"32244af61f3644f09d1579b052a7ec3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e39562c83fc4fdd9ae5cf55d1e041f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"22b9fd542cd24d59bae3c19c143437d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6ee8921178847e08b9ad0854cd7753f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fbbaabe2699a452089503525f9c34c9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2acb8fa61fab4ea7a98fa87340fa075a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8e04eca8c9bb437fac125fd871aa93fa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9b1835e57a0e4f5eaef731235f54b353","IPY_MODEL_9ddcf6529cbd4126bc11367107fd979a","IPY_MODEL_2421c01201f14a19858bab34d52dde00"]}},"8e04eca8c9bb437fac125fd871aa93fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b1835e57a0e4f5eaef731235f54b353":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6cdfe1814158464c86a04d467a70dd80","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4757384a8cee42ac98bcf5b4c035b0e7"}},"9ddcf6529cbd4126bc11367107fd979a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_937f2611234e479ea779bf96bb8f01c7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa291becb0584820ae00c4dedcb2919f"}},"2421c01201f14a19858bab34d52dde00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e693fcf6cacd48a080f6a9cfc67464e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:00&lt;00:00, 16.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_621ec3144103493896fe077668542a78"}},"6cdfe1814158464c86a04d467a70dd80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4757384a8cee42ac98bcf5b4c035b0e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"937f2611234e479ea779bf96bb8f01c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fa291becb0584820ae00c4dedcb2919f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e693fcf6cacd48a080f6a9cfc67464e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"621ec3144103493896fe077668542a78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a1429d0febe4fcd92f12643336bf543":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8c9f286bfdf045de8760ac8be20909bd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0e359a7528e74e069ed7e54ab7fe67a3","IPY_MODEL_34a7130461e84e8580ab3ad337515b6e","IPY_MODEL_accfc711ae884645a2f9c151aeb8df6d"]}},"8c9f286bfdf045de8760ac8be20909bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e359a7528e74e069ed7e54ab7fe67a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d2f60f37dc7f482e88713631caa8cb8a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd345b306bce47f9a5319001ce987e1d"}},"34a7130461e84e8580ab3ad337515b6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e21e6890730a4b57bebb77fd865ef980","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_337801181d8f4dc3bc7705439cfeb656"}},"accfc711ae884645a2f9c151aeb8df6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d4d590de38c4c20a13441785f87dfe0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 681M/681M [00:30&lt;00:00, 56.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_37c0c430e3dd452b9dea5ebfaf1d46ad"}},"d2f60f37dc7f482e88713631caa8cb8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd345b306bce47f9a5319001ce987e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e21e6890730a4b57bebb77fd865ef980":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"337801181d8f4dc3bc7705439cfeb656":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d4d590de38c4c20a13441785f87dfe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"37c0c430e3dd452b9dea5ebfaf1d46ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":5}